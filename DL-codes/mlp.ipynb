{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知机 Perceptron\n",
    "给定输入x，权重w和偏移b，感知机输出：\n",
    "$$\n",
    "o = \\sigma(<w, x> + b), \\sigma(x) = \n",
    "\\begin{cases}\n",
    "    1, if x > 0\\\\\n",
    "    -1, otherwise\n",
    "\\end{cases}\n",
    "$$\n",
    "**二分类**\n",
    "\n",
    "\n",
    "* 训练感知机——等价于使用batch_size=1的梯度下降，使用损失函数：\n",
    "$$\n",
    "\\mathcal{l}(y, x, w) = max(0, -y<w, x>)\n",
    "$$\n",
    "\n",
    "```\n",
    "initialize w = 0, b = 0\n",
    "repeat\n",
    "    if y[i](<w, x[i]> + b) then\n",
    "        w = w + y[i]x[i]\n",
    "        b = b + y[i]\n",
    "    end if\n",
    "until all classified correctly\n",
    "```\n",
    "\n",
    "* 收敛定理\n",
    "    * 数据在半径r内\n",
    "    * 余量rho分类两类\n",
    "        $$\n",
    "            y(x^T w + b) \\ge \\rho\n",
    "        $$\n",
    "        对于$||w||^2 + b^2 \\le 1$\n",
    "    * 感知机保证在$\\frac{r^2 + 1}{\\rho^2}$步后收敛\n",
    "\n",
    "* XOR问题\n",
    "感知机不能拟合XOR函数，只能产生线性分割面(需要核函数用核支持向量机)————导致第一次AI寒冬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多层感知机MLP\n",
    "*  学习XOR——使用两层感知机\n",
    "\n",
    "* 单隐藏层——隐藏层大小是一个超参数\n",
    "#### 单分类\n",
    "* 输入$x \\in R^n$\n",
    "* 隐藏层$W_1 \\in R^{m \\times n}, b_1 \\in R^m$\n",
    "* 输出层$w_2 \\in R^m, b_2 \\in R$\n",
    "    $$\n",
    "    h = \\sigma(W_1 x + b)\\\\\n",
    "    o = w_2^Th + b_2\n",
    "    $$\n",
    "    其中sigma是按元素的激活函数\n",
    "![Hidden Layer](./imgs/hidden-layer.png)\n",
    "\n",
    "* 为什么需要激活函数\n",
    "如果没有激活函数，12, 23层整体作用结果是一个线性函数，那整体还是一个线性模型，和一层没有区别好的吧\n",
    "\n",
    "\n",
    "### 激活函数\n",
    "* Sigmoid激活函数\n",
    "    * 将输入投影到(1, 0)，是一个软的激活函数\n",
    "    $$\n",
    "    \\sigma(x) = \n",
    "    \\begin{cases}\n",
    "    1, if~x > 0\\\\\n",
    "    0, other wise\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "    $$\n",
    "    sigmoid(x) = \\frac{1}{1 + exp(-x)}\n",
    "    $$\n",
    "    ![sigmoid](./imgs/sigmoid.png)\n",
    "\n",
    "* tanh激活函数\n",
    "    * 将输入投影到(-1, 1)\n",
    "    $$\n",
    "    tanh(x) = \\frac{1 - exp(-2x)}{1 + exp(-2x)}\n",
    "    $$\n",
    "    ![tanh](./imgs/tanh.png)\n",
    "* ReLU激活函数\n",
    "    * ReLU: rectified linear unit\n",
    "    $$\n",
    "    ReLU(x) = max(x, 0)\n",
    "    $$\n",
    "    ![ReLU](./imgs/ReLU.png)\n",
    "\n",
    "**注意，指数运算是很贵的，算一次指数几乎相当于**\n",
    "\n",
    "#### 多分类\n",
    "$y_1, y_2, ..., y_k = softmax(o_1, o_2, ..., o_k)$\n",
    "加个隐藏层然后再做softmax回归\n",
    "* 输入$x \\in R^n$\n",
    "* 隐藏层$W_1 \\in R^{m \\times n}, b_1 \\in R^m$\n",
    "* 输出层$W_2 \\in R^{m \\times k}, b_2 \\in R^k$\n",
    "    $$\n",
    "    h = \\sigma(W_1x + b_1)\\\\\n",
    "    o = W_2^Th + b_2\\\\\n",
    "    y = softmax(o)\n",
    "    $$\n",
    "\n",
    "可以做多隐藏层，超参数：隐藏层数&每层隐藏层的大小\n",
    "\n",
    "注意提取特征需要慢慢压缩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP代码实现\n",
    "#### 手搓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现一个具有单隐藏层的MLP，包含256个隐藏单元\n",
    "\n",
    "num_inputs, num_outputs, num_hiddens = 28 * 28, 10, 256\n",
    "\n",
    "W1 = nn.Parameter(\n",
    "    torch.randn(num_inputs,num_hiddens, requires_grad=True)\n",
    ")\n",
    "b1 = nn.Parameter(\n",
    "    torch.zeros(num_hiddens, requires_grad=True)\n",
    ")\n",
    "W2 = nn.Parameter(\n",
    "    torch.randn(num_hiddens, num_outputs, requires_grad=True)\n",
    ")\n",
    "b2 = nn.Parameter(\n",
    "    torch.zeros(num_outputs, requires_grad=True)\n",
    ")\n",
    "\n",
    "params = [W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现relu几乎函数\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现模型\n",
    "def net(X):\n",
    "    X = X.reshape((-1, num_inputs))\n",
    "    H = relu(X @ W1 + b1)\n",
    "    return (H @ W2 + b2)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'd2l.torch' has no attribute 'train_ch3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m num_epochs, lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m0.1\u001b[39m\n\u001b[1;32m      4\u001b[0m updater \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(params, lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m----> 5\u001b[0m \u001b[43md2l\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ch3\u001b[49m(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'd2l.torch' has no attribute 'train_ch3'"
     ]
    }
   ],
   "source": [
    "# 训练过程同前面的spftmax回归的训练过程\n",
    "\n",
    "num_epochs, lr = 10, 0.1\n",
    "updater = torch.optim.SGD(params, lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
