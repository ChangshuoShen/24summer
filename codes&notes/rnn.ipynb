{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 序列模型\n",
    "\n",
    "### 序列数据\n",
    "* 实际中很多数据是有时序结构的\n",
    "* 电影的评价随时间变化而变化\n",
    "    * 拿奖后评分上升，直到奖项被忘记\n",
    "    * 看了很多好电影后，人们的期望变高\n",
    "    * 季节性：贺岁片、暑期档\n",
    "    * 导演、演员的负面报道导致评分变低\n",
    "    * ...\n",
    "\n",
    "\n",
    "### 统计工具\n",
    "* 在时间t观察到$x_t$，那么得到T个不独立的随机变量\n",
    "$$\n",
    "(x_1, ..., x_T) \\sim p(x)\n",
    "$$\n",
    "* 使用条件概率展开\n",
    "$$\n",
    "p(a, b) = p(a)p(b|a) = p(b)p(a|b)\n",
    "$$\n",
    "展开的大一点\n",
    "$$\n",
    "p(x) = p(x_1)p(x_2 | x_1)p(x_3|x_1, x_2)...p(x_T|x_1, ..., x_{T-1})\n",
    "$$\n",
    "反过来也可以展开，但是物理上不一定可行\n",
    "### 序列模型\n",
    "$$\n",
    "p(x) = p(x_1)p(x_2 | x_1)p(x_3|x_1, x_2)...p(x_T|x_1, ..., x_{T-1})\n",
    "$$\n",
    "* 对条件概率建模\n",
    "$$\n",
    "p(x_t| x_1, x_2, ..., x_{t - 1}) = p(x_t | f(x_1, ..., x_{t - 1}))\n",
    "$$\n",
    "对见过的数据建模，也称`自回归模型`\n",
    "\n",
    "#### 方案A——Markov假设\n",
    "* 假设当前数据只跟$\\tau$个过去数据点相关\n",
    "$$\n",
    "p(x_t | x_1, ..., x_{t - 1}) = p(x_t | x_{t - \\tau}, ..., x_{t - 1}) = p(x_t| f(x_{t - \\tau}, ..., x_{t - 1}))\n",
    "$$\n",
    "例如在过去数据上训练一个MLP模型\n",
    "\n",
    "#### 潜变量\n",
    "* 引入浅$h_t$来表示过去信息$h_t = f(x_1, ..., x_{t - 1})$\n",
    "    * 这样$x_t = p(x_t | h_t)$\n",
    "\n",
    "summary\n",
    "* 时序模型中，当前数据跟之前观察到的数据相关\n",
    "* 自回归模型使用自身过去数据来预测未来\n",
    "* Markov模型假设当前值跟最近少数数据相关，从而简化模型\n",
    "* 浅变量模型使用潜变量来概括历史信息\n",
    "\n",
    "\n",
    "### 代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 序列模型\n",
    "# 使用正弦函数和一些可加性噪声生成序列数据，时间步为1,2,...，1000\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "T = 1000\n",
    "time = torch.arange(1, T + 1, dtype=torch.float32)\n",
    "x = torch.sim(0.01 * time) + torch.normal(0, 0.2, (T,))\n",
    "d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 将数据映射为数据对y_t = x_t和x_t = [x_{t - \\tau}, ..., x_{t - 1}]\n",
    "\n",
    "tau = 4 # 我们的模型假设每一个时间步的x只和过去的4个时间步有关\n",
    "features = torch.zeros((T - tau, tau))\n",
    "\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: T - tau + i]\n",
    "\n",
    "labels = x[tau: ].reshape((-1, 1))\n",
    "\n",
    "betch_size, n_train = 16, 600\n",
    "\n",
    "train_iter = d2l.load_array(\n",
    "    (features[: n_train], labels[: n_train]),\n",
    "    batch_size, is_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 使用一个简单的结构：只是一个拥有两个全连接层的MLP\n",
    "\n",
    "def init_wieghts(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential(\n",
    "        nn.Linear(4, 10), nn.ReLU(),\n",
    "        nn.Linear(10, 1)\n",
    "    )\n",
    "    net.apply(init_wieghts)\n",
    "    return net\n",
    "\n",
    "loss = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "\n",
    "def train(net, train_iter, loss, epochs, lr):\n",
    "    trainer = torch.optim.Adam(net.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        for X, y in train_iter:\n",
    "            trainer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "        print(\n",
    "            f'epoch {epoch + 1}, ',\n",
    "            f'loss: {d2l.evaluate_loss(net, train_iter, loss)}'\n",
    "        )\n",
    "\n",
    "net = get_net()\n",
    "train(net, train_iter, loss, 5, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "onestep_preds = net(features)\n",
    "d2l.plot(\n",
    "    [time, time[tau:]],\n",
    "    [x.detach().numpy(), onestep_preds.detach().numpy()],\n",
    "    'time', 'x',\n",
    "    legend=['data', '1-step preds], xlim=[1, 1000], figsize=(6, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 进行多步预测\n",
    "\n",
    "multistep_preds = torch.zeros(T)\n",
    "multistep_preds[: n_train + tau] = x[: n_train + tau]\n",
    "for i in range(n_train + tau, T):\n",
    "    multistep_preds[i] = net(multistep_preds[i - tau: i].reshape((1, -1)))\n",
    "\n",
    "d2l.plot(\n",
    "    [time, time[tau: ], time[n_train + tau]],\n",
    "    [\n",
    "        x.detach().numpy(), \n",
    "        onestep_preds.detach().numpy(), \n",
    "        multistep_preds.detach().numpy()    \n",
    "    ],\n",
    "    'time', 'x', legend=['data', '1-step preds', 'multistep preds'],\n",
    "    xlim=[1, 1000], figsize=(6, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 在仔细看一下k步预测\n",
    "\n",
    "max_steps = 64\n",
    "\n",
    "features = torch.zeros((T - tau - max_steps + 1, tau + max_steps))\n",
    "\n",
    "for i in range(tau):\n",
    "    features[:, i] = x[i: i + T - tau - max_steps + 1]\n",
    "\n",
    "for i in range(tau, tau + max_steps):\n",
    "    features[:, i] = net(features[:, i - tau: i]).reshape(-1)\n",
    "\n",
    "steps = (1, 4, 16, 64)\n",
    "\n",
    "d2l.plot(\n",
    "    [time[tau + i - 1: T - max_steps + i] for i in steps],\n",
    "    [features[:, (tau + i - 1)].detach().numpy() for i in steps]\n",
    "    'time', 'x', legend=[f'{i}-step preds' for i in steps],\n",
    "    xlim=[5, 1000], figsize=(6, 3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本预处理\n",
    "\n",
    "### 代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re # 正则表达式的module\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "# 将数据集读取到由多条文本组成的列表中\n",
    "\n",
    "def read_time_machine():\n",
    "    '''Load the time machine dataset into a list of text lines'''\n",
    "    with open(d2l.download('time_machine', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 实际使用中不会用这么暴力的预处理\n",
    "    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
    "\n",
    "lines = read_time_machine()\n",
    "\n",
    "print(f'text lines: {len(lines)}')\n",
    "print(lines[0])\n",
    "print)lines[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 每个文本序列又被拆分成一个标记列表\n",
    "\n",
    "def toeknize(lines, token='word):\n",
    "    '''将文本行拆分成单词或字符进行标记'''\n",
    "    if token == 'word':\n",
    "        return [line.split() for line in lines]\n",
    "    elif token == 'char':\n",
    "        return [list(line) for line in lines]\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "tokens = toeknize(lines)\n",
    "for i in range(11):\n",
    "    print(tokens[i])\n",
    "\n",
    "\n",
    "# 构建一个字典，通常叫做词汇表vocabulary，用来将字符串类型的标记映射到从0开始的数字索引中\n",
    "\n",
    "class Vocab:\n",
    "    '''文本词汇表'''\n",
    "    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
    "        if tokens is None:\n",
    "            tokens = []\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens = []\n",
    "        counter = count_freqs(tokens)\n",
    "        self.token_freqs = sorted(\n",
    "            counter.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        self.unk, uniq_tokens = 0, ['<unk>'] + reserved_tokens\n",
    "        # unk是一个常见的表示，意思是unknown\n",
    "\n",
    "        uniq_tokens += [\n",
    "            token for token, freq in self.token_freqs\n",
    "            if freq >= min_freq and token not in uniq_tokens\n",
    "        ]\n",
    "        self.idx_to_token, self.token_to_idx = [], dict()\n",
    "        for token in uniq_tokens:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token] = len(self.idx_to_token)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_token(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "def count_corpus(tokens):\n",
    "    '''统计标记的频率'''\n",
    "    if len(tokens) == 0 or isinstance(tokens[0], list):\n",
    "        tokens = [token for line in tokens for token in line]\n",
    "    return collections.Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 构建词汇表\n",
    "vocab = Vocab(tokens)\n",
    "print(list(vocab.token_to_idx.items())[: 10])\n",
    "\n",
    "# 将每一行文本行转换成一个数字索引列表\n",
    "for i in [0, 10]:\n",
    "    print('words:', tokens[i])\n",
    "    print('indices:', vocab[tokens[i]])\n",
    "\n",
    "# 将所有功能打包到load_corpus_time_machine函数种\n",
    "def load_corpus_time_machine(max_tokens=-1):\n",
    "    '''返回时光机器数据集的标记索引列表和词汇表'''\n",
    "    lines = read_time_machine()\n",
    "    tokens = tokenize(lines, 'char')\n",
    "    vocab = Vocab(tokens)\n",
    "    # corpus语料库： a collection of written or spoken material stored on a computer and used to find out how language is used\n",
    "    corpus = [vocab[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[: max_tokens]\n",
    "    return corpus, vocab\n",
    "\n",
    "corpus, vocab = load_corpus_time_machine()\n",
    "len(corpus), len(vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 语言模型\n",
    "* 给定文本序列$x_1, ..., x_T$，语言模型的目标是估计联合概率$p(x_1, ..., x_T)$\n",
    "* 它的应用包括：\n",
    "    * 做预训练模型(BERT, GPT-3等)\n",
    "    * 做文本生成，给定前几个词，不断使用$x_t \\sim p(x_t|x_1, ..., x_{t -1})$来生成后续文本\n",
    "    * 判断多个序列中哪个更常见\n",
    "\n",
    "### 使用计数来建模\n",
    "* 假设序列长度为2,预测\n",
    "$$\n",
    "p(x, x') = p(x)p(x'|x) = \\frac{n(x)}{n} \\frac{x(x, x')}{n(x)}\n",
    "$$\n",
    "这里n是总词数，$n(x), n(x, x')$是单个单词和连续单词对的出现次数\n",
    "* 很容易拓展到长为3的情况\n",
    "$$\n",
    "p(x, x', x'') = p(x)p(x'|x)p(x''|x, x') = \\frac{n(x)}{n} \\frac{n(x, x')}{n(x)} \\frac{n(x, x', x'')}{n(x, x')}\n",
    "$$\n",
    "### N元语法\n",
    "* 当序列很长时，因为文本量不够大，很可能$n(x_1, ..., x_T) \\le 1$\n",
    "* 使用Markov假设来缓解这个问题：\n",
    "\n",
    "* 一元语法\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4) = p(x_1)p(x_2)p(x_3)p(x_4) = \\frac{n(x_1)}{n} \\frac{n(x_2)}{n} \\frac{n(x_3)}{n} \\frac{n(x_4)}{n}\n",
    "$$\n",
    "* 二元语法\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4) = p(x_1)p(x_2 | x_1)p(x_3|x_2)p(x_4|x_3) = \\frac{n(x_1)}{n} \\frac{n(x_1, x_2)}{n(x_1)} \\frac{n(x_2, x_3)}{n(x_2)} \\frac{n(x_3, x_4)}{n(x_3)}\n",
    "$$\n",
    "* 三元语法\n",
    "$$\n",
    "p(x_1, x_2, x_3, x_4) = p(x_1)p(x_2 | x_1)p(x_3 | x_1, x_2)p(x_4 | x_1, x_2, x_3)\n",
    "$$\n",
    "\n",
    "### 代码——语言模型和数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch \n",
    "from d2l import troch as d2l\n",
    "\n",
    "tokens = d2l.tokenize(d2l.read_time_machine())\n",
    "corpus = [token for line in lines for token in line]\n",
    "vocab = d2l.Vocab(corpus)\n",
    "vocab.token_freqs[: 10]\n",
    "\n",
    "\n",
    "# 最流行的词，被称为`停用词`，画出词频图\n",
    "freqs = [freq for token, freq in vocab.token_freqs]\n",
    "d2l.plot(\n",
    "    freqs, xlabel='token: x', ylabel='frequency: n(x)',\n",
    "    yscale='log'\n",
    ")\n",
    "\n",
    "# 其他的词元组合，比如二元语法，三元语法\n",
    "bigram_tokens = [\n",
    "    pair for pair in zip(corpus[: -1], corpus[1:])\n",
    "]\n",
    "bigram_vocab = d2l.Vocab(bigram_tokens)\n",
    "print(bigram_vocab.token_freqs[: 10])\n",
    "\n",
    "trigram_tokens = [\n",
    "    triple for triple in \n",
    "    zip(corpus[: -2], corpus[1: -1], corpus[2:])\n",
    "]\n",
    "trigram_vocab = d2l.Vocab(trigram_tokens)\n",
    "print(trigram_vocab.token_freqs[: 10])\n",
    "\n",
    "# 直观的对比三种模型中的标记频率\n",
    "bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n",
    "trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n",
    "d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n",
    "         ylabel='frequency: n(x)', xscale='log', yscale='log',\n",
    "         legend=['unigram', 'bigram', 'trigram'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 随机的生成一个小批量数据的特征和标签以供读取。\n",
    "# 在随机采样中，每个样本都是在原始的长序列上任意捕捉的字序列\n",
    "\n",
    "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
    "    '''使用随机抽样生成一个小批量子序列'''\n",
    "    corpus = corpus[random.randint(0, num_steps - 1):]\n",
    "    num_subseqs = (len(corpus) - 1) // num_steps\n",
    "    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
    "    random.shuffle(initial_indices)\n",
    "\n",
    "    def data(pos):\n",
    "        return corpus[pos: pos + num_steps]\n",
    "    \n",
    "    num_batches = num_subseqs // batch_size\n",
    "    for i in range(0, batch_size * num_batches, batch_size):\n",
    "        # 在这里，initial_indices包含子序列的随机起始索引\n",
    "        initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
    "        X = [data(j) for j in initial_indices_per_batch]\n",
    "        Y = [data(j + 1) for j in initial_indices_per_batch]\n",
    "        yield torch.tensor(X), torch.tensor(Y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 比如说生成一个0～34的序列\n",
    "\n",
    "my_seq = list(range(35))\n",
    "for X, y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n",
    "    print(f'X: {X}, y: {y}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 保证两个相邻的小批量的子序列在原始序列上也是相邻的\n",
    "\n",
    "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
    "    '''使用顺序分区生成一个小批量子序列'''\n",
    "    offset = random.randint(0, num_steps)\n",
    "    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
    "    Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
    "    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
    "    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)\n",
    "    num_batches = Xs.shape[1] // num_steps\n",
    "    for i in range(0: num_steps * num_batches, num_steps)\n",
    "        X = Xs[:, i: i + num_steps]\n",
    "        Y = Ys[:, i: i + num_steps]\n",
    "        yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 读取每个小批量的子序列的特征X和标签Y\n",
    "\n",
    "for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n",
    "    print(f'X: {X}, Y: {Y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 将上面的两个采样函数包装到一个类中\n",
    "\n",
    "class SeqDataLoader:\n",
    "    '''加载序列数据的迭代器'''\n",
    "    def __init__(self, batch_size, num_steps,\n",
    "                 use_random_iter, max_tokens):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn = d2l.seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn = d2l.seq_data_iter_sequential\n",
    "        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n",
    "        self.batch_size, self.num_steps = batch_size, num_steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 最后定义一个函数load_data_time_machine，同时返回数据迭代器和词汇表\n",
    "\n",
    "def load_data_time_machine(batch_size, num_steps,\n",
    "                            use_random_iter=False, max_tokens=10000):\n",
    "    '''返回时光机器数据集的迭代器和词汇表'''\n",
    "    data_iter = SeqDataLoader(batch_size, num_steps, \n",
    "                            use_random_iter, max_tokens)\n",
    "    return data_iter, data_iter.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Recurrent NN 循环神经网络\n",
    "\n",
    "### 潜变量回归模型\n",
    "* 使用潜变量(latent variable)$h_t$总结过去信息\n",
    "$$\n",
    "p(h_t | h_{t - 1}, x_{t - 1})\\\\\n",
    "p(x_t | h_t, x_{t - 1})\n",
    "$$\n",
    "\n",
    "* hidden variable是现实世界中真实存在的问题，只是观察不到\n",
    "* latent variable可以用来指代现实生活中不存在的东西\n",
    "\n",
    "### 循环神经网络\n",
    "* 更新隐藏状态：$ h_t = \\phi (W_{hh}h_{t - 1} + W_{hx}x_{t - 1} + b_h $\n",
    "    * 如果去掉其中的$W_{hh$h_{t - 1}$就退化成呢各MLP\n",
    "* 输出：$o_t = \\phi (W_{ho}h_t + b_o)$\n",
    "![rnn](./imgs/rnn.png)\n",
    "\n",
    "#### 使用循环神经网络的语言模型\n",
    "\n",
    "### 困惑度(perplexity)\n",
    "* 衡量一个语言模型的好坏可以用平均交叉熵\n",
    "$$\n",
    "\\pi = \\frac{1}{n}\\Sigma_{i = 1}^n -logp(x_t | x_{t - 1}, ...)\n",
    "$$\n",
    "p是语言模型的预测概率，$x_t$是真实词\n",
    "* 历史原因NLP使用困惑度$exp(\\pi)$来衡量，是平均每次可能选项\n",
    "    * 1表示完美，无穷大是最差情况\n",
    "\n",
    "### 梯度裁减\n",
    "* 迭代中计算这T个时间步上的梯度，在反向传播过程中产生长度为O(T)的矩阵乘法链，导致数值不稳定\n",
    "* 梯度裁减能有效预防梯度爆炸\n",
    "    * 如果梯度长度超过$\\theta$，那么拖影回长度$\\theta$\n",
    "$$\n",
    "g \\leftarrow min(1, \\frac{\\theta}{||g||})g\n",
    "$$\n",
    "\n",
    "### 更多应用\n",
    "* 文本生成\n",
    "* 文本分类\n",
    "* 问答、机器翻译\n",
    "* Tag生成\n",
    "\n",
    "### RNN代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F \n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
